# Comparativa de Procesamiento de Datos con Apache Spark: Scala vs PySpark

Este proyecto muestra una implementaci√≥n paralela de ejercicios pr√°cticos de procesamiento de datos usando Apache Spark, desarrollados en:

- ‚öôÔ∏è **Scala + Spark** (ejecutado localmente con IntelliJ)
- ‚òÅÔ∏è **Python + PySpark** (ejecutado en entorno remoto, con datos desde AWS)

El objetivo es **comparar los enfoques, ventajas y diferencias pr√°cticas** de ambos lenguajes en contextos reales de trabajo con big data.

---

##  Estructura del repositorio

```
.
‚îú‚îÄ‚îÄ ExamenNauzet.scala      # Versi√≥n en Scala + Spark
‚îú‚îÄ‚îÄ ExamenNauzetTest.scala  # Tests automatizados de la versi√≥n Scala
‚îú‚îÄ‚îÄ TestInit.scala          # Infraestructura de testing (proporcionada)
‚îú‚îÄ‚îÄ build.sbt               # Configuraci√≥n del proyecto Scala
‚îú‚îÄ‚îÄ ExamenNauzet.py         # Versi√≥n en Python + PySpark
‚îú‚îÄ‚îÄ AWS                     # Carpeta con im√°genes configuracion en AWS
‚îú‚îÄ‚îÄ Scala vs Pyspark        # Carpeta con im√°genes comparando c√≥digo 
‚îú‚îÄ‚îÄ ventas.csv              # Archivo CSV de prueba
‚îî‚îÄ‚îÄ README.md               # Este archivo
```

---

##  Ejercicios Implementados

| Ejercicio | Descripci√≥n                                                                 |
|----------|-----------------------------------------------------------------------------|
| **1**     | Crear un DataFrame de estudiantes, filtrar por calificaci√≥n, ordenar       |
| **2**     | UDF para determinar si un n√∫mero es par o impar                            |
| **3**     | Join entre estudiantes y sus calificaciones + c√°lculo del promedio         |
| **4**     | Conteo de ocurrencias de palabras usando RDDs                              |
| **5**     | Lectura de archivo CSV y c√°lculo de ingresos por producto                  |

---

##  Comparativa: Scala + Spark vs PySpark

| Caracter√≠stica            | Scala + Spark                                       | Python + PySpark                                       |
|--------------------------|----------------------------------------------------|--------------------------------------------------------|
| **Lenguaje**             | Tipado est√°tico, ejecutado en la JVM               | Din√°mico, m√°s flexible                                 |
| **Curva de Aprendizaje** | M√°s pronunciada, ideal para desarrolladores        | M√°s accesible para cient√≠ficos de datos y analistas    |
| **Entorno de trabajo**   | Frecuente en desarrollo local(on-prem o cloud)     | Altamente integrado en plataformas cloud          |
| **Performance**          | Muy alto, especialmente en entornos Spark puros    | Excelente en cl√∫steres distribuidos en la nube         |
| **Casos de Uso**         | Backend de datos, ETLs complejos,pipe empresariales| Ciencia de datos, prototipos, pipelines cloud-native   |

---

##  Infograf√≠a Visual

<img width="1720" alt="2025-04-11 13_41_17-" src="https://github.com/user-attachments/assets/2af766f8-9638-4c13-9a10-7acd14bafdac" />


---

##  Conclusiones

Despu√©s de trabajar con ambas implementaciones, puedo decir que **tanto Scala como Python se integran muy bien con Apache Spark** y permiten sacarle mucho partido a esta tecnolog√≠a.

Aunque pueda parecer lo contrario, **personalmente me ha resultado m√°s sencillo trabajar con Scala**, ya que ofrece de forma nativa algunas herramientas y estructuras que encajan muy bien con la l√≥gica de Spark.

Aun as√≠, **PySpark tambi√©n tiene sus ventajas**, especialmente en entornos cloud o cuando se trabaja con equipos orientados a ciencia de datos.

En definitiva, como suele pasar en estos temas, **la mejor respuesta es: ‚Äúdepende‚Äù**.

Depende del proyecto, del equipo, del entorno de trabajo y del tipo de tareas que se quieran resolver. Lo importante es entender las fortalezas de cada enfoque y elegir en funci√≥n de lo que se necesita.

---

##  Requisitos

### Para correr la versi√≥n en Scala
- IntelliJ IDEA
- Scala SDK 2.12.x o 2.13.x
- Apache Spark instalado localmente


### Para correr la versi√≥n en PySpark
- Python 3.8+
- PySpark instalado (`pip install pyspark`)
- Acceso a AWS S3

---

## Autor

**Nauzet**  
 Contacto: Nauzet.fdez@gmail.com

---

> Este repositorio busca servir como base de comparaci√≥n y aprendizaje entre dos formas de trabajar con Spark. ¬°Gracias por pasarte por aqu√≠! üöÄ

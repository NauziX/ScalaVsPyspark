# Comparativa de Procesamiento de Datos con Apache Spark: Scala vs PySpark

Este proyecto muestra una implementaci√≥n paralela de ejercicios pr√°cticos de procesamiento de datos usando Apache Spark, desarrollados en:

- ‚öôÔ∏è **Scala + Spark** (ejecutado localmente con IntelliJ)
- ‚òÅÔ∏è **Python + PySpark** (ejecutado en entorno remoto, con datos desde AWS)

El objetivo es **comparar los enfoques, ventajas y diferencias pr√°cticas** de ambos lenguajes en contextos reales de trabajo con big data.

---

## üìÅ Estructura del repositorio

```
.
‚îú‚îÄ‚îÄ ExamenNauzet.scala      # Versi√≥n en Scala + Spark
‚îú‚îÄ‚îÄ ExamenNauzet.py         # Versi√≥n en Python + PySpark
‚îú‚îÄ‚îÄ AWS                     # Carpeta con im√°genes configuracion en AWS
‚îú‚îÄ‚îÄ Scala vs Pyspark        # Carpeta con im√°genes comparando c√≥digo 
‚îú‚îÄ‚îÄ ventas.csv              # Archivo CSV de prueba
‚îî‚îÄ‚îÄ README.md               # Este archivo
```

---

## ‚úÖ Ejercicios Implementados

| Ejercicio | Descripci√≥n                                                                 |
|----------|-----------------------------------------------------------------------------|
| **1**     | Crear un DataFrame de estudiantes, filtrar por calificaci√≥n, ordenar       |
| **2**     | UDF para determinar si un n√∫mero es par o impar                            |
| **3**     | Join entre estudiantes y sus calificaciones + c√°lculo del promedio         |
| **4**     | Conteo de ocurrencias de palabras usando RDDs                              |
| **5**     | Lectura de archivo CSV y c√°lculo de ingresos por producto                  |

---

## ‚öñÔ∏è Comparativa: Scala + Spark vs PySpark

| Caracter√≠stica            | Scala + Spark                                       | Python + PySpark                                       |
|--------------------------|----------------------------------------------------|--------------------------------------------------------|
| **Lenguaje**             | Tipado est√°tico, ejecutado en la JVM               | Din√°mico, m√°s flexible                                 |
| **Curva de Aprendizaje** | M√°s pronunciada, ideal para desarrolladores        | M√°s accesible para cient√≠ficos de datos y analistas    |
| **Entorno**              | Local (IntelliJ IDEA), ficheros locales            | Cloud-ready (VSCode, S3), m√°s escalable                |
| **Performance**          | Muy alto, especialmente en entornos Spark puros    | Excelente en cl√∫steres distribuidos en la nube         |
| **Casos de Uso**         | Producci√≥n backend, procesamiento batch complejo   | Ciencia de datos, prototipos, pipelines cloud-native   |

---

## üì∑ Infograf√≠a Visual

<img width="1720" alt="2025-04-11 13_41_17-" src="https://github.com/user-attachments/assets/2af766f8-9638-4c13-9a10-7acd14bafdac" />


---

## üìå Conclusiones

- Ambas implementaciones permiten explotar la potencia de Apache Spark de forma efectiva.
- **Scala + Spark** destaca en entornos productivos robustos y estructuras empresariales grandes.
- **PySpark** es m√°s usado en ciencia de datos, aprendizaje autom√°tico, y trabajos en la nube.
- La elecci√≥n de la tecnolog√≠a depende del equipo, la infraestructura y el flujo de trabajo.

---

## üì¶ Requisitos

### Para correr la versi√≥n en Scala
- IntelliJ IDEA
- Scala SDK 2.12.x o 2.13.x
- Apache Spark instalado localmente


### Para correr la versi√≥n en PySpark
- Python 3.8+
- PySpark instalado (`pip install pyspark`)
- Acceso a AWS S3 (para lectura del CSV remoto)

---

## üßë‚Äçüíª Autor

**Nauzet**  
üìß Contacto: Nauzet.fdez@gmail.com

---

> Este repositorio busca servir como base de comparaci√≥n y aprendizaje entre dos formas de trabajar con Spark. ¬°Gracias por pasarte por aqu√≠! üöÄ

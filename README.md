# Comparativa de Procesamiento de Datos con Apache Spark: Scala vs PySpark

Este proyecto muestra una implementaciÃ³n paralela de ejercicios prÃ¡cticos de procesamiento de datos usando Apache Spark, desarrollados en:

- âš™ï¸ **Scala + Spark** (ejecutado localmente con IntelliJ)
- â˜ï¸ **Python + PySpark** (ejecutado en entorno remoto, con datos desde AWS S3)

El objetivo es **comparar los enfoques, ventajas y diferencias prÃ¡cticas** de ambos lenguajes en contextos reales de trabajo con big data.

---

## ğŸ“ Estructura del repositorio

```
.
â”œâ”€â”€ ExamenNauzet.scala      # VersiÃ³n en Scala + Spark
â”œâ”€â”€ ExamenNauzet.py         # VersiÃ³n en Python + PySpark
â”œâ”€â”€ AWS                     # Carpeta con imÃ¡genes configuracion en AWS
â”œâ”€â”€ Scala vs Pyspark        # Carpeta con imÃ¡genes comparando cÃ³digo 
â”œâ”€â”€ ventas.csv              # Archivo CSV de prueba
â””â”€â”€ README.md               # Este archivo
```

---

## âœ… Ejercicios Implementados

| Ejercicio | DescripciÃ³n                                                                 |
|----------|-----------------------------------------------------------------------------|
| **1**     | Crear un DataFrame de estudiantes, filtrar por calificaciÃ³n, ordenar       |
| **2**     | UDF para determinar si un nÃºmero es par o impar                            |
| **3**     | Join entre estudiantes y sus calificaciones + cÃ¡lculo del promedio         |
| **4**     | Conteo de ocurrencias de palabras usando RDDs                              |
| **5**     | Lectura de archivo CSV y cÃ¡lculo de ingresos por producto                  |

---

## âš–ï¸ Comparativa: Scala + Spark vs PySpark

| CaracterÃ­stica            | Scala + Spark                                       | Python + PySpark                                       |
|--------------------------|----------------------------------------------------|--------------------------------------------------------|
| **Lenguaje**             | Tipado estÃ¡tico, ejecutado en la JVM               | DinÃ¡mico, mÃ¡s flexible                                 |
| **Curva de Aprendizaje** | MÃ¡s pronunciada, ideal para desarrolladores        | MÃ¡s accesible para cientÃ­ficos de datos y analistas    |
| **Entorno**              | Local (IntelliJ IDEA), ficheros locales            | Cloud-ready (VSCode, S3), mÃ¡s escalable                |
| **Performance**          | Muy alto, especialmente en entornos Spark puros    | Excelente en clÃºsteres distribuidos en la nube         |
| **Casos de Uso**         | ProducciÃ³n backend, procesamiento batch complejo   | Ciencia de datos, prototipos, pipelines cloud-native   |

---

## ğŸ“· InfografÃ­a Visual



---

## ğŸ“Œ Conclusiones

- Ambas implementaciones permiten explotar la potencia de Apache Spark de forma efectiva.
- **Scala + Spark** destaca en entornos productivos robustos y estructuras empresariales grandes.
- **PySpark** es mÃ¡s usado en ciencia de datos, aprendizaje automÃ¡tico, y trabajos en la nube.
- La elecciÃ³n de la tecnologÃ­a depende del equipo, la infraestructura y el flujo de trabajo.

---

## ğŸ“¦ Requisitos

### Para correr la versiÃ³n en Scala
- IntelliJ IDEA
- Scala SDK 2.12.x o 2.13.x
- Apache Spark instalado localmente


### Para correr la versiÃ³n en PySpark
- Python 3.8+
- PySpark instalado (`pip install pyspark`)
- Acceso a AWS S3 (para lectura del CSV remoto)

---

## ğŸ§‘â€ğŸ’» Autor

**Nauzet**  
ğŸ“§ Contacto: Nauzet.fdez@gmail.com

---

> Este repositorio busca servir como base de comparaciÃ³n y aprendizaje entre dos formas de trabajar con Spark. Â¡Gracias por pasarte por aquÃ­! ğŸš€

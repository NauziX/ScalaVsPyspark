# Comparativa de Procesamiento de Datos con Apache Spark: Scala vs PySpark

Este proyecto muestra una implementaci贸n paralela de ejercicios pr谩cticos de procesamiento de datos usando Apache Spark, desarrollados en:

- 锔 **Scala + Spark** (ejecutado localmente con IntelliJ)
- 锔 **Python + PySpark** (ejecutado en entorno remoto, con datos desde AWS)

El objetivo es **comparar los enfoques, ventajas y diferencias pr谩cticas** de ambos lenguajes en contextos reales de trabajo con big data.

---

##  Estructura del repositorio

```
.
 ExamenNauzet.scala      # Versi贸n en Scala + Spark
 ExamenNauzetTest.scala  # Tests automatizados de la versi贸n Scala
 TestInit.scala          # Infraestructura de testing (proporcionada)
 build.sbt               # Configuraci贸n del proyecto Scala
 ExamenNauzet.py         # Versi贸n en Python + PySpark
 AWS                     # Carpeta con im谩genes configuracion en AWS
 Scala vs Pyspark        # Carpeta con im谩genes comparando c贸digo 
 ventas.csv              # Archivo CSV de prueba
 README.md               # Este archivo
```

---

##  Ejercicios Implementados

| Ejercicio | Descripci贸n                                                                 |
|----------|-----------------------------------------------------------------------------|
| **1**     | Crear un DataFrame de estudiantes, filtrar por calificaci贸n, ordenar       |
| **2**     | UDF para determinar si un n煤mero es par o impar                            |
| **3**     | Join entre estudiantes y sus calificaciones + c谩lculo del promedio         |
| **4**     | Conteo de ocurrencias de palabras usando RDDs                              |
| **5**     | Lectura de archivo CSV y c谩lculo de ingresos por producto                  |

---

##  Comparativa: Scala + Spark vs PySpark

| Caracter铆stica            | Scala + Spark                                       | Python + PySpark                                       |
|--------------------------|----------------------------------------------------|--------------------------------------------------------|
| **Lenguaje**             | Tipado est谩tico, ejecutado en la JVM               | Din谩mico, m谩s flexible                                 |
| **Curva de Aprendizaje** | M谩s pronunciada, ideal para desarrolladores        | M谩s accesible para cient铆ficos de datos y analistas    |
| **Entorno de trabajo**   | Frecuente en desarrollo local(on-prem o cloud)     | Altamente integrado en plataformas cloud          |
| **Performance**          | Muy alto, especialmente en entornos Spark puros    | Excelente en cl煤steres distribuidos en la nube         |
| **Casos de Uso**         | Backend de datos, ETLs complejos,pipe empresariales| Ciencia de datos, prototipos, pipelines cloud-native   |

---

##  Infograf铆a Visual

<img width="1720" alt="2025-04-11 13_41_17-" src="https://github.com/user-attachments/assets/2af766f8-9638-4c13-9a10-7acd14bafdac" />


---

##  Conclusiones

- Ambas implementaciones permiten explotar la potencia de Apache Spark de forma efectiva.
- **Scala + Spark** destaca en entornos productivos robustos y estructuras empresariales grandes.
- **PySpark** es m谩s usado en ciencia de datos, aprendizaje autom谩tico, y trabajos en la nube.
- La elecci贸n de la tecnolog铆a depende del equipo, la infraestructura y el flujo de trabajo.

---

##  Requisitos

### Para correr la versi贸n en Scala
- IntelliJ IDEA
- Scala SDK 2.12.x o 2.13.x
- Apache Spark instalado localmente


### Para correr la versi贸n en PySpark
- Python 3.8+
- PySpark instalado (`pip install pyspark`)
- Acceso a AWS S3 (para lectura del CSV remoto)

---

## Autor

**Nauzet**  
 Contacto: Nauzet.fdez@gmail.com

---

> Este repositorio busca servir como base de comparaci贸n y aprendizaje entre dos formas de trabajar con Spark. 隆Gracias por pasarte por aqu铆! 
